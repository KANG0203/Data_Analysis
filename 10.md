## 3pg.

Ensemble methods
- training data에서 base classifier(or weak learner) set를 만든다.
- testing instacne의 class label을 예측하기 위해 base claaifiers가 내린 예측에 대한 과반수 투표를 한다

## 4 ~ 5pg.

Why Ensembles?  

25개의 base classifier가 있다고 해보자.  
각 classifier는 error rate가 0.35이다.  
만약 base classifiers가 동일하다면, ensemble base classifiers가 잘못 예측한 같은 예제들을 잘못 분류한 것이다.  
base classifiers가 independent하다면(오류가 uncorrelated), ensemble은 base classifier들 중 반 이상이 잘못 예측한 경우에만 잘못된 예측을 수행한다.  
-> 결국 ensemble을 사용하면 잘못 예측할 확률이 줄어든다는 소리  

ensemble classifier가 잘못 예측할 확률 식 확인만 하자  

그래프를 보자
- base classifier가 동일한 경우 대각선으로, independet한 경우 실선임

앙상블 조건
- base classifier가 서로 independent
- base classifier는 random 추측을 수행하는 classifier보다 성능이 더 좋아야 한다.(오차<0.5)
- 실제로는 완벽히 independent 어려움, but 약간 상관되어 있을 때 앙상블 써도 성능 향상

## 7pg.

Bagging
- bootstrapping은 resampling의 한 유형으로, ranndom sample with replacement(랜덤 복원추출?)를 사용하여 원래 dataset과 동일한 크기의 dataset을 생성
- bagging은 반복적으로 bootstrap sample D를 생성하고, 각 D에 대한 classifier 생성
- 각 D는 original data와 같은 size
- 동일한 training set에 일부 instance는 여러 번 나타날 수 있고, 다른 instance는 누락될 수 있다
- D는 original data의 63%정도가 포함된다.
- 각 data object는 D의 size가 m일때 1 - (1 - 1/m)^m의 확룰로 선택된다.

## 8 ~ 10pg.

Decision stump - one-level binary decision tree로 테스트 조건은 x<=k  
예시에서 bagging이 없는 경우 분할은 x<=0.35 or x<= 0.75가 될 것이며 최대 정확도는 70%이다.  
- if x<=0.35 then y = 1면 0.8, 0.9, 1이 틀리고, if x<=0.75 then y = -1이면 0.1,0.2,0.3이어서 정확도는 70%이다.

예시 보면 10번 bootstrap sample 생성해서 decision stump 만들어서 예측하고, 각 예측을 모아서 투표해서 결정한다.  

## 11pg.

bagging의 성능은 base classifier의 stability에 따라 달라진다. -> 햇갈리니 잘 암기
- unstable하면 bagging은 error를 줄이는데 도움이 된다
- stable하면 bagging은 오히려 성능을 저하시킬 수 있다.

bagging은 noise data에 적용될 때 model의 overfitting에 민감하지 않는다.  

## 12pg.

rnadom forest
- 다양한 decision tree 생성하는데 중점
- 원래 dataset에서 bootstrap 샘플 D 생성
- D에서 란덤으로 선택된 small subset of features(# of features)으로 decision tree를 구축
- 반복
- 예측은 투표 기반

## 14 ~ 25pg.

Boosting
- traning data의 분포를 조정하기 위한 iterative procedure으로, 이전에 misclassified records에 더 중점을 둔다
- 처음에는 모든 m record에 동일한 가중치를 준다
- bagging과 달리 boosting round의 끝에서 가중치가 변경될 수 있다.
- 가중치는 원래 data에서 bootstrap sample 집합을 추출하는데 sampling 분포로 사용될 수 있다
- 또 base classifier에 의해 높은 자중치 example을 향한 편향된 모델을 학습하는데 사용될 수 있다.

잘못된 분류된 record의 가중치는 증가, 올바르게 분류된 record의 가중치는 감소.  
예시를 보면 example 4는 분류되기 어려워 가중치가 증가해, 다음 라운드에서 다시 선택될 가능성이 높아진다.  

boosting 알고리즘은
- 각 라운드 끝에서 training example의 갖우치가 어떻게 업데이트 되는지
- 각 classifier에 의해 예측이 어떻게 결합되는지
에 따라 차이가 있다.

## 16 ~ 17pg.

AdaBoost
- 있는 식 대충 보기

weight update for each example
- 같으면 더 작은값, 다르면 더 큰 값, 즉 분류가 잘 안되면 가중치를 더 준다
classification
- 가중치를 곱해서 더한 값ㅇ르 더해서 argmax로 가장 최대로 많드는 값 리턴?

낮은 정확도를 가진 모델에 penalize  
중간 라운드 중에서 error rate가 50%를 넘어가면 가중치는 1/n으로 되돌리고 resampling 절차 반복  

## 18 ~ 19pg.

예시를 보자
- 처음에는 1/n로 가중치를 준다
- round가 지날때마다 예측이 맞으면 가중치를 낮추고, 틀리면 올린다

## 20pg.

Gradient Boost regression
- target variable의 평균을 계산하고, 예측값이 평균인 경우 residual(=observed-predicted)을 예측하는 1st tree 구축
- 두번째 tree를 구축하여 residual을 예측하는데, predicted = average + learing rate * 1st tree prediction이다
- 위와 같음
 - 새로운 data를 예측하기 위해 모든 tree의 예측을 더함 (learning rate는 각 tree에 곱해짐)

Gradient Boost classification
- regression과 유사하지만, residual이 log, sigmoid 함수 등을 사용하여 더 복잡하게 계산된다

## 21pg.

XGBoost (Extreme Gradient Boost)
- gradient bosst와 비슷하지만 다음과 같은 차이가 있음

각 regression(or classification) tree를 구축하는 디테일  
다양한 최적화 기술 
- for huge dataset: approximate greedy algorithm, parallel learning, weighted quantile sketch
- for missing value: sparsity-aware split finding
- Hardware-aware optimizations: cache-aware access, blocks for out-of-core computation

## 23pg.

Feature selection using Ensemble of decision trees  

feature 중요도 계산 방법
- 각 node에 대해 해당 node가 책임지는 observations의 수를 가중치로 적용하여 성능 측정을 계산하고, 모든 트리에 대해 이를 평균화한다
- OOB(out of bag) observation에서 wrapper method 수행

## 26pg.

Linear classifier (for binary classification)
- linear funcion은 object x에 대해 ppt에 있는 식으로 분류한다.
- wi는 가중치
- b(w0)는 bias(scalar)
- x는 f(x)>0이면 positive, <0이면 negative로 분류

## 27pg.

Linear classifier: Geometric intuition
- R^2이면 line, R^3이면 plane, R^4 이상이면 hyperplane
- 예시를 보면 (2,0)은 positive, (0,2)는 negative, (2,4)는 0인데 어디로 분류되는거지

## 28pg.

Linear, quadratic, polynomial classifiers  

linear func - 각각 feature의 가중치만 고려  
quadratic func - 두 feature을 연결한 것의 가중치도 고려  
polynomial function - 여러 feature을 연결한 것의 가중치 고려  

## 29 ~ 31pg.

Perceptron algorithm
- perceptron이란 linear func를 학습하기 위한 알고리즘이다.
- 1. f를 randomly setting w로 초기화
  2. 각 training point x에 대해, f가 x를 misclassifies하면 w를 x를 올바르게 분류하는 방향으로 조정
  3. 2번 반복
 
Nondeterministic
- 동일한 training set에 대해 data 순서에 따라 다른 f 생성될 수 있다
- 하이퍼파라미터 조정(반복 횟수, w 얼마나 조정할지)

Limited to linear functions
- 다항 함수에 대한 w의 수는 Nondeterministic 방식으로 훈련하기에 너무 많다
- 다항 함수를 훈련하는데는 본질적으로 O(n^p)가 필요하다
- SVM은 O(n)안에 임의의 차수의 다항 함수 학습할 수 있다.

## 32pg.

Multi-Layer Perceptron (MLP)
- 시그마는 activation function으로 불린다. -> 전통적으로는 sigmoid 함수가 사용되었지만, 현재는 ReLU(활성화 함수) 시그마(z) = max{z,0}이 가장 널리 사용된다
- v랑 w는 learning parameters (backpropagaton으로 학습)
- multiple neurons는 multiclass classification에 사용된다.

## 33pg.

Deep learning / Artificial Neural Network (ANN)
- 걍 슥 읽어봐도 될듯하다
