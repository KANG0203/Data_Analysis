## 3pg.

PageRank  

Early Search Engines and Term Spam
- search query -> inverted index를 이용하여 term을 포함하는 page 찾기, headear, body에서 term의 빈도에 따라 순위 매기기
- 어떻게 속일까

Google’s two innovations - 다른 사람이 그에 대해 말하는 걷을 듣다
- page에 link된 근처 term을 사용하여 rank 매기기 -> spammer들은 종종 자신의 page에 link된 page를 제어 못함
- in-links 수를 어떻게 셀까 -> "spam farm" problem
- PageRank: random sufers가 random page에서 시작하여 randomly chosen outlink를 따라감 -> 자주 방문되는 page가 드물게 방문되는 page보다 중요

왜 Pagerank가 작동할까
- user는 좋거나 유용한 page에 link를 추가하는 경향이 있다
- random suffer는 이러한 page를 방문하는 경향이 있다.

## 4pg.

Basic PageRank  

Starting probability V0
- sufer가 다음에 각 page에 있을 확률
- 모든 page가 1/n으로 동일
- Vi = Mi * V0

transiton matrix M
- 열 기준으로 한 노드가 나머지 행에 해당하는 노드로 갈 확률

## 5pg.

Markov process  

만약 strongly connected(어디든 도착 가능)이면 V = M*V로 수렴  

V는 M의 e-vector
- M * V = 람다 * V
- M은 확률 행렬이여서 열의 합이 1이고, V는 principal e-vector이다.(e=vealue는 1이고, 가장 큼)
- M의 e-vector는 suffer가 오랜 시간 후에 어디 있는지 알려준다
- V0부터 50~ 75번 반복하면 실제로 수렴하는데 충분

밑에 행렬 보면 첫번째는 V0, 두번째는 V1, ... 마지막은 V인데 이는 V = M*V를 만족하는 수렴한 V이다.  

## 6pg.

Structure of the Web
- in-component: SCC(Strongly connected component)에 도달하지만, SCC로부터 도달되지 않는다
- out-component: SCC로부터 도달하지만, SCC로 도달하지 않는다.
- Tendrils, Tubes, Isolated components
- sufer는 out-component 및 tendrils로 이동하게 되며 -> in-component나 SCC는 중요하지 않다

## 7pg.

Avoiding dead
- Recursively drop dead ends
- Taxation

EX>
- 우선 dead ends를 드랍한다.
- in만 있는 녀석 drop, 우선 E 제거하면 C도 in만 있어 C도 제거
- 그리고 나머지 3개의 노드로 Markov process 진행
- C로 가는 A와 D에 대해 각각 C로 갈 확률이 1/3, 1/2이므로 곱해서 C 확률 구함
- E는 C 확률과 같음

## 8pg.

Taxation and spider trap
- V' = B*M*V + (1-B)e/n -> e/n은 random jump
- B는 0.8~0.9 사이 상수
- e는 1 vector, n은 node 수

EX>
- spider trap은 dead end는 아니지만 밖으로 나가는 link가 없는 경우고 예시에서는 노드 C에 해당한다
- 만약 taxation을 안하면 pagerank는 C만 1이고 나머지는 0이 된다
- taxation을 적용하면 B가 0.8인 경우 예시에서 보는 것처럼 v'이 나오고 이와 같이 계산한다.

## 9pg.

PageRank scoring formula  
- D는 모든 web page의 set이다
- I(p)는 p로 가는 page의 set, |Q(q)|는 q에서 나가는 page의 set이다.
- pagerank score PR(p)는 우선 d와 곱해져 있는 놈을 보면 자신의 가리키고 있는 모든 page q를 nomalize시키고 합친다
- 1-d와 곱해진 놈은 random jump를 나타낸다.

## 10pg.

Link spam  

spam farm의 architecture
- 접근 불가 page: spammer에 의해 영향 안받음
- 접근 가능 page: spammer에 의해 영향 받을 수 있지만 제어되지 않음
- own page: spammer에 의해 제어 ㄱㄴ

## 11pg.

How to prevent link spam
- 한 page가 많은 page에 link하고 다시 그 page로 link되는 구조를 감지하고 제거한다 -> spammer는 다양한 구조로 변형한다
- PageRank의 정의 수정하여 link spam의 rank 낮춤
- TrustRank: topic-sensitive PageRank의 변형
- Spam mass: Spam과 유사한 page를 식별하기 위해 계산

## 12pg.

TrustRank
- 신뢰할 수 있는 것으로 여겨지는 page set을 topic으로 하는 topic-sensitive PageRank
- PageRank의 상위 목록에서 topic 선택
- ".edu"와 같은 통제된 domain 사용

Spam Mass = (PageRank - TrustRank) / PageRank
- Spam Mass 가 높다 -> trustrank가 낮다 -> spam 확률 높다

## 13 ~ 14pg.

Hubs and authorities: HITS (hyperlink-induced topic search)
- Authorities: 특정 topic에 대한 정보를 제공하는 가치 있는 page
- Hubs: Authorities에 대한 link를 제공하는 가치 있는 page
- page는 good hub인 경우 good authorities를 link하며, good authorities이면 good hub에 의해 링크된다
- h(hubbiness score)와 a(authority score)를 반복적으로 계산
- h와 a vector는 모두 1 vector로 시작
- a = L^t h 밑 가장 큰 component를 1로 scaling, La = h
- change가 작아질 때까지 반복

예시 함 보자  
- h에서 시작하여 L^t 곱하고 정규화해서 a 구하고, a에 L 곱하고 정규화해서 h구하고를 반복

