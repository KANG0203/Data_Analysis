## 2pg.

time series
- 연속적인 시간 동안 측정되는 순차적인 data point 집함

Time-series analysis
-  data의 의미 있는 통계와 기타 특성을 추출하기 위해 time series data를 분석하는 방법 포함

## 3pg.

key ingredient
- trend and seasonal variation
- stationarity(안정성)
- autocorrelation and partial autocorrelation

예측 위한 전통 방법: AR, MA, ARMA, ARIMA  
Machine learning: time series 예측을 위한 XGBoost  
딥러닝: RNN, CNN, transformer  

## 5pg.

Three components of time series
- trend, seasonal, irregular
- Additive model: 3요소 독립
- Multiplicative model: 3요소 독립 아닐 수 있다.

## 7 ~ 9pg.

plms에서 코드 다운받을 수 있다.  

여러 열중에 여기서는 'Montreal_Median_Price'에 대하여 본다.  

그리고 위에서 보았던 Multiplicative와 additive에 대해 seasonal_decompose함수를 사용하였다.  

seasonal_decompose 함수는 time series성분을 분해하는 것이다.  

보면 trend, seasonal, residual로 나누어지는 것을 볼 수 있는데, residual은 treand와 seasonl을 제거한 나머지 값이다.  

## 10 ~ 11pg.

Stationarity
- time series는 const mean and variance를 가지면 stationary이다.
- AR,MA,ARMA, ARIMA는 stationary time series에 적합

정상성은 시간에 무관하게 과거, 현재, 미래의 분포가 같아야 한다는 의미
- 10pg. 그래프에서 a가 stationary process
- trend나 seasonality가 없어야 한다.

## 12 ~ 13pg.

Unit Roots
- 식 보기

세타가 1보다 작은 경우에만 Stationary하고, 아닌 경우에는 non-startionary하다.  

왜냐하면 평균 분산 둘다 상수여야 하는데 세타가 1보다 같거나 크면 아니게 된다.  

## 14 ~ 15pg.
Dickey-Fuller test  

Null hypothesis: 세타 = 1 -> non-stationary  
Alternative hypothesis: 세타 < 1 -> stationary  

식 변형된걸로 쓰자. 델타에 대해서 봐야함  

델타 = 세타 -1  
그러면 세타가 아니라 델타 = 0 이면 non, 델타 < 0이면 stationary  

t-statistic을 통해서 DFcritical보다 작으면 reject H0, 크면 fail to reject  

## 16pg.

Augmented Dickey-Fuller (ADF) test  

걍 인자가 1이 아니라 p일때임.  

## 17pg.

time-serices가 stationary라면 -> traditional method를 결정하기 위해 ACF와 PACF 확인  

## 18pg.

ACF는 direct와 indrect를 모두 고려, PACF는 direct만 고려한다.  

## 20pg.

ACF가 decline gradually, PACF가 급격히 감소 -> AR  
ACF가 급격히 감소, PACF가 decline gradually -> MA  
둘다 decline gradually -> ARMA  

## 21pg.

Time series prediction
- 예측 위한 전통 방법: AR, MA, ARMA, ARIMA  
- Machine learning: time series 예측을 위한 XGBoost  
- 딥러닝: RNN, CNN, transformer  

## 22pg.

Autoregressive Model: AR(p)
- output variable은 이전 값 및 stochasitc term에 linerly 의존
- AR(p)의 PACF는 p의 유의한 지연 이후에 급격한 감소를 보이는 반면, ACF는 점진적으로 감소
- a0 ~ ap는 argmin으로 결정 -> error를 최소화하는 a를 선택

## 23pg.

Moving average model: MA()
- output variable은 현재 및 다양한 과거 값들에 stochastic(불완전하게 예측 가능한) 항에 lineraly 의존
- 식 보면 평균에 과거 에러들에 b 곱한거에 현재 error 더함
- 예제 보기

![image](https://github.com/KANG0203/Data_Analysis/assets/144585363/3d16f170-3415-4530-adc8-1b79d929459c)

## 24pg.

AR model
- p를 결정하기 위해 PACF 사용
- 예측하기 위해 과거의 x 값 본다

MR model
- q를 결정하기 위해 ACF 사용
- 예측하기 위해 과거의 error 값 본다

## 25pg.

Autoregressive Moving Average Model: ARMA(p, q)
- ARMA(p,q) = AR(p) + MA(q)
- PACF로 p, ACF로 q 선택

## 26pg.

Autoregressive Integrated Moving Average Model: ARIMA(p, d, q)
- ARMA 모델의 일반화로, data가 sense of mean에서 non-stationarity를 보이는 경우 적용
- 예를 들어 time-series 자체를 예측하는 대신, time-series에서 한 time-stamp에서 이전 time-stamp까지의 차이 예측

## 27pg.

XGBoost for time series prediction
- 걍 슥 봄

## 30pg.

Fourier transform
- 길이가 T인 time-series는 3 coefficient로 나타낼 수 있다(3차원 vector)
- 현실에서는 잡음 많아서 이렇게 분해 잘 안된다
- 딥 러닝은 model이 time series를 처리하는 방법을 learing하게 해줌

## 31pg.

Deep learning for time series prediction
- nonlinear sequence(ordering) 관계 학습 가능
- single time stamp뿐만 아니라 sequence의 예측도 잘 수 행(AR과 같은 예측에서는 error가 잘 전파된다. 딥러닝에서는 고정 길이의 sequence 예측 잘 다룬다)
- multivariate time-series에 대해 잘 처리
- 임의의 변수 추가 가능

## 32pg.

Preprocessing time-series
- Normalization: 각 채널 독립적으로 표준화
- Imputation: linear or spline func으로 interpolate
- Denoising: "rolling means"로 각 채널을 smooth, FFT로 high frequency 제거
- Decomposition: trend 제거

