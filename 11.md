## 2pg.

SVM(support vector machine)
- nonlinear func을 학습하기 위해 "kernel trick"을 사용
- 튜닝할 하이퍼파라미터 적다
- learned in batch mode(한번에 처리)

ANN
- onlinear func을 학습하기 위해 multi-layers 사용
- 튜닝할 하이퍼파라미터 많다
- 점층적으로 learned 가능

## 3pg.

Linear, quadratic, polynomial classifiers에 대해서는 앞 ppt에서 다루었다.  
SVM은 "kernel trick"을 사용하여 특정 features에 대한 linear time내에 임의의 임의의 차수의 다항 함수 학습  
그래프를 보면 degree가 커질수록 training set의 eroor는 작아진다. test set은 작아지다 점점 높아진다.  
degree가 커질수록 overfitting  

## 4 ~ 6pg.

Linear SVM
- data를 분리하는 liner hyperplane(decision boundary)를 찾는다.

그림에서 B1과 B2중 뭐가 더 좋을까? goodness는 어떻게 측정할까?
- margin을 최대로 하는 hyperplane이 더 좋다
- 여기서 margin이란 hyperplane과 data 사이의 만나지 않는 최대 거리? 같은 느낌인 듯
- 그래서 B1이 더 좋다

## 7pg.

labeled dataset D가 주어진다.
- xi는 data vector이고 yi는 class label인데 +1 or -1이다.

D가 "linearly seperable" 가능하다면
- D 내의 모든 data vector을 정확하게 분류하는 linear func f(x) = w*x가 존재하는 것이다
- 즉 모든 D에 속한 (xi,yi)에 대해 (w*xi)yi>0을 만족하는 w가 존재하는 것이다.

그러면 (w'*xi)yi >= 1을 만족하는 w'가 존재할까?  

## 9pg.

margin = 1/|w|를 최대로 해야 한다. -> |w|를 최소화 해도 된다  
대신 우리는 L(w) = |w|^2/2를 최소화 할 것이다.  
모든 D에 속한 (xi,yi)에 대해 (w*xi)yi>=1을 만족해야 하는 제약이 있다.
- 이는 제약이 있는 최적화 문제이다
- 이를 해결하기 위해 numerical approaches를 한다.(ex> quadratic programming)

## 10pg.

Linear SVM (Soft margin)
- What if not linearly separable?
- 여기서 slack variable을 도입한다
- slack variable은 자신이 속한 class에서 margin까지의 거리로 slack>1이면 잘못 분류, slack=0이면 잘 분류, <slack<1이면 마진 안쪽에 존재하는 경우이다 
- 식은 알아서 보자

## 11 ~ 12pg.

앞에서 봤던 식은 priaml form이다. 이를 dual form으로 바꾸면 저렇게 된다.  
dual form에서는 w를 교체한 듯 하다.  

많은 ai가 0이며 w는 small number of data points의 linear combination이다. (support vectors)  
ai가 0이 아닌 xi는 suppot vectors라 불린다.  
decision bounday는 support vectors에 의해서만 결정된다.  

## 13pg.

SVM model: support vectors와 계수 a의 목록  
C가 모델에 어떻게 영향을 미칠까? -> a 값을 조정해서 흠.. 몰루  
dual로 변환하는 이유가 뭘까? w를 찾는 것보다 계수인 a를 찾는게 더 쉬워서  

## 14 ~ 15pg.

그림을 보면 원래는 decision boundary가 nonlinear인데 고차원 공간으로 data를 바꾸니 linear로 바뀜.  

What if decision boundary is nonlinear?  

Naive way
- 고차원 공간으로 data 변환
- 새로운 feature space에서 linear func 계산 (새로운 space의 linear는 원래 space에서는 nonlinear)

SVM kernel trick
- 이 과정은 data를 고차원으로 변홚지 않고 수행

## 16 ~ 18pg.

kernel trick
- xi * xj 식을 k(xi,xj) 로 바꿈
- k(x,y) = (1+x*y)^p (degree p)

K(xi,xj)는 xi와 xj가 similar하면 higher value를 return  
data가 K 함수에서만 나타난다 -> x의 form의 제약이 없고, 이는 vector 이상일 수 있다.(ex> sequence, tree, graph)  

## 19pg.

Nonlinear SVM: Popularly used kernels
- Polynomial kernel: K(x,y) = (1+x*y)^p (degree p)
- Radial basis function (RBF) kernel: K(x,y) = e^(-r||x-y||^2)
- r이 움직일 때, boundary shape가 움직이며, 이것에 대한 튜닝이 부적절하면 overfitting이나 underfitting 일어날 수 있다.

두번째 그림 overfitting, 3번째 underfitting  

## 20pg

Nonlinear SVM: Kernels  

모든 similarity measure가 kernel func로 사용되는건 아니다
- kernel func는 Mercer 함수를 만족해야 한다. 즉 positive definite를 가져야 한다
- m by m kernel 행렬, 즉 (i,j) 번째 entry가 k(xi,xj)인 경우 항상 positive definite

## 21pg.

Tuning hyperparameters in SVM  

Soft margin parameter C
- C가 0에 접근하면 soft, 증가하면 hard

Polynomial kernel parameter p
- p가 증가하면 boundary가 더 복잡
- p=1에서 시작하여 1씩 증가
- 일반화 성능은 어느 point에서 향상이 멈춘다

RBF kernel parameter r
- 좋은 휴리스틱이 없다
- 작은 수에서 시작하여(2^-6) 각 반복에서 2를 곱하여 2^6까지 진행
- r의 좋은 범위를 찾으면 해당 범위 내에서 미세하게 조절

## 22pg.

SVM implementations  

LIBSVM
- 매우 최적화된 구현
- 많은 언어 도구에 대해 interface 제공
- 다양한 유형의 SVM 지원 ex> multi class classification, nu-SVM, one-class SVM
- 매우 빠른 linear SVM도 지원 즉 LIBLinear

SVM-light
- 가장 쉬운 구현
- ranking SVM 지원

## 23pg.

SVM: Multiclass classification
- a set of binary classification으로 축소될 수 있다
- One-to-all
- Pairwise coupling(or one-to-one): SVM에서 더 많이 사용된다

## 24pg.

One-to-all  

k Class classification를 위해 k binary classifiers를 생성
- 각 binary classifier는 하나의 class를 positive로, 나머지 class를 negative로 training
- 새로운 object를 분류할 때 classifiers의 majority vote로 결정
적은 수의 binary classifiers가 생성
각 classifier의 training set이 더 크다.

Pairwise coupling (or one-to-one)  

K classes의 각 pair마다 binary classifier 생성
- K! / 2!(k-2)! 의 binary classifier 생성
- 각 classifier는 두 class의 data에서 구성 하나는 pos 다른 하나는 neg
- 새로운 object를 분류할 때 classifiers의 majority vote로 결정
더 많은 binary classifiers가 생성
각 classifier의 training set이 더 작다


## 25 ~ 26pg.

SVM rank learning  

notation
- (xi > xj): 어떤 순서에 따라 xi가 랭크가 더 높다
- (xi,xj)ㅌR: ordering R에 따라 xi가 랭크가 더 높다
- R' = {(xi > xj)}는 user에게 제공된 data의 partial ordering이다
- R*은 user에게 알려지지 않은 데이터의 원하는 global ordering이다

Ranking model
- input: a set of partial orders R'
- output: 모든 (xi,xj)ㅌR'에 대해 f(xi) > f(xj)를 만족하는 f(x)

linear func: f(x) = w*x
- 모든 (xi,xj)ㅌR'에 대해 f(xi) > f(xj)를 만족은 w*xi > w*xj와 같다.
- 이러한 w가 존재한다면 R'은 linearly rankable

일반화를 고려한 목표
- input: a set of partial orders R'(R*에 속함)
- output: 모든 (xi,xj)ㅌR*에 대해 f(xi) > f(xj)를 만족하는 f(x) -> R'이 R*로 바뀜

우리는 partical orders R'에서 f를 훈련하고(w를 계산하고), 도한 R'과 관련하여 R*에 대해 보지 못한 데이터를 잘 일반화할 수 있기를 원한다.  

## 27pg.

RankSVM
- 최소화 식은 같음
- 조건 식 확인하자

## 28

여기서 람다는 최소 랭킹 스코어이다.  
람다가 더 큰데 magin이 큰거여서 더 좋은것 -> w1  

## 29gg.

- nonlinear ranking func를 learn하기 위해 kernel trick 적용할 수 있나?
- RankSVM구현 -> SVM light
