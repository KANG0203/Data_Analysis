## 3pg.

IF-THEN rules for classification  

IF-THEN으로 나타냄  

rule 평가 -> coverage and accuracy
- n_covers = # of tuples covered by R
- n_correct = # of tuples correctly classified by R
- coverage(R) = n_covers/ |D|, D는 데이터 집합
- accuracy = n_corrrect/n_covers

여러 rule이 triggered된 경우 출돌 해결 필요
- size ordeing : "toughest(가장 엄격한)" requirement를 가진 triggering rules에 가장 높은 우선순위 부여(ex> 가장 많은 속성 테스트를 포함하는 경우)
- class-based ordering : class당 prevalence(발생률)이나 misclassification cost를 내림차순으로
- rule-based ordering(decision list) : rules는 전문가나 rule quality의 측정에 따라 하나의 long priority list 목록으로 구성

## 4pg.

첫번째 rule은 coverage는 100이지만 accuracy가 낮다. 3번째 rule는 coverage가 줄지만 accuracy가 높아진다.  
rule를 더 추가하여 perfect rule set을 얻을 수 있다.  

## 5pg.

Rule induction from data: Sequential covering algorithm  

Sequential covering algorithm: training data에 직접 rule 추출 (ex> FOIL, RIPPER)  
각각의 class Ci에 대해 순차적으로 규칙을 학습
- 가장 작은 class에 대해 rule set을 먼저 학습(in ripper)
- 각 rule은 가능한 한 Ci의 tuple을 많이 포함하면서 다른 class의 tuple은 가능한 한 적게 포함

Step
- rule은 한번에 하나씩 학습
- 각 rule이 학습되면 rule에 의해 covered된 tuple은 제거
- 종료 조건이 충족될 때까지 남은 tuples에서 process가 반복(ex> 더 이상 training example이 없을 때, quality of rule returned가 사용자 지정 threshold 아래로 떨어질 때)

## 6 ~ 9pg.

while (enough target tuples left)  
  generate a rule  
  remove positive target tuples satisfying this rule  

예시 보면 규칙을 점점 추가해서 positive example에만 속하게 하여 accuracy를 높힌다.  

## 7pg.

일반적인 rule로 시작: condition = empty  
greedy depth-first strategy를 채택하여 반복적으로 새 predicate 추가 -> rule quality를 가장 향상시키는 predicate 선택  

rule-quality measures: coverage, accuracy 고려  
- Foil-gain(in Ripper) : 조건을 확장하여 얻는 이득 평가
- pos'+neg' : # of instances covered by rule
- pos' : # of instacne coverd by rule that belong to the positive class
- 높은 accuracy, 많은 positive tuple cover하는 predicate가 좋음

rule R이 구성되면, R이 reduced error pruning(오류 줄이는 가지치기?)에 의해 제거될 수 있다.
- reduced error pruning : R은 validation set에 의해 pruned
- Measure of pruning : pos-neg / pos + neg
- pruning on a validation set 후 FOL-prune이 더 크면 pruned된다

## 11 ~ 13pg.

desicion tree에 의해 생성된 rule은 모든 data를 cover하며, each rule은 independent해서 ordering이 중요하지 않다.(mutual exclusion)  
하지만 rule based는 ordering이 중요.  

예시를 보면 decision truee에 의해 만들어진 rules를 ordered해서 새로 만들어내는 것을 볼 수 있다.  

## 14pg.

decision tree에서 한 rule 추가하려면 그림과 같이 동일한 rule을 여러개 복제해서 추가하는데 이러면 복잡해진다.  

## 15pg.

Decision tree and rules  

둘다 interpretable
- rule set이 더 명확하고 이해하기 쉬울 수 있다.
- rules는 independent하지 않다.(ordering 중요) -> decision tree는 각 rule이 independent

multiclass situation
- rule learning을 위한 covering algorithm은 한번에 한 class에 집중
- decision tree는 모든 class를 고려

Theoretically, rules and trees have equivalent “descriptive power”  
- 그러나 rule는 "not-equally-empowered" decision list, 순서대로 진행되어 하나가 발동될 때 까지 진행

rule은 순차적으로 실행되기 때문에, 각각의 규칙은 그 앞의 규칙들과 함께 해석되어야 한다.  

## 17pg.

k-nearest neighbor(KNN)
- 주어진 test instance x에 대해 x의 knn을 찾고, 다수결 투표로 예측 -> 첫번째 그림 보면 -3개 + 2니깐 -(5-nearest neighbor)
- 1-NN decision boundary는 Boronoi diagram이 된다.(두번째 그림)
- 너무 작은 k면 overfitting, 너무 큰 k면 underfitting, k가 커지면 boundary는 sommther

## 18pg.

kNN for regression
- k개의 nearest neighbors의 mean value를 return

Distance-weighted nearest neighbor algorithm:
- 더 가까운 neighbor에 더 큰 가중치 부여

## 19pg.

knn은 lazy learning(instance-based learning)이다. -> 별도의 model learning이 필요 없
- 단순히 모든 training data를 저장
- k개의 nearest neighbors를 기반으로 classify
- training은 빠르지만 예측은 느림

the others
- training data로 부터 model 구축
- model을 사용하여 classify
- training은 느리지만 예측은 빠름

KNN은 예측에서 'ideal'한 정확도를 얻을 수 있지만, "accurate"한 proximity func이 필요하다.  

## 22pg.

Bayes’ theorem  

trainging data X에 대해 가설 H의 posterior 확률 P(H|X)는 다음 공식을 따른다.

P(H|X) = P(X|H)P(H)/P(X), 여기서 posterior(사후) 확률이란 P(H)가 prior(사전) 확률 이고, X를 반영한 H의 확률을 P(H|X)라 한다.

그래서 위 식을 posterior = likehood(기능도) * prior / evidence(증거 or 정규화 상수) 로 표현한다.  

만약 P(Ck|X) 중에서 P(Ci|X)가 가장 높다면 X가 Ci에 속한다고 예측한다.  

## 23pg.

Naïve Bayes classifier
- posterior 확률을 계산하려면 많은 확률에 대한 initial 지식이 필요하다
- 간단한 가정 : attribute는 조건부로 독립적이다. (즉 attribute간의 의존 관계 없다) 그래서 식을 저렇게 쓰는듯?
- 위의 가정은 계산 비용을 크게 줄인다
- Ak가 categorical하면, P(xk|Ci)는 Ak를 |Ci|로 나눈 값 xk를 가지는 Ci의 # of tuple이다.
- Ak가 continuous하면 P(xk|Ci)는 밑에 식처럼 계산되는듯

## 24 ~ 25pg.

일단 보면 각 class에 대한 확률을 구한다.  
그리고 위에서 attribute가 독립이라 가정해서, 각 attribute의 like hood를 각자 계싼해서 총 likehood를 구한다.  
이를 class 확률로 곱해서 뭐가 더 큰지 구한다.  

식들 보면 이해 감  

## 26pg.

Laplacian smoothing: Avoiding 0-probability problem  

Naïve Bayesian 예측은 각 조건부 확률이 0이 아니어야 한다. 0이면 예측된 확률이 0이 되어 버린다.  
laplacian smoothing은 이러한 일을 막기 위해 각 case별로 1을 더해준다. 그러면 조건부 확률이 0인 경우가 생기지 않는다  

## 27pg.

Pros
- 구현 쉽고 결과 좋다

Cons
- 가정 - 글래스 조건부 독립성을 가정했어서, 이때매 정확도 손실 발생
- 실제로 변수 간에 dependence가 존재
- attributes간의 dependenc는 Naïve Bayesian으로 모델링 할 수 없다.

